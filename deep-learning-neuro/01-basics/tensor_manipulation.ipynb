{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor manipulation using `TensorLy`\n",
    "\n",
    "Modified from: Jean Kossaifi's https://github.com/JeanKossaifi/tensorly-notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tensory_logo](../assets/TensorLy_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be using the `pytorch`backend\n",
    "\n",
    "Check https://pytorch.org/get-started/locally<br>\n",
    "`conda install pytorch torchvision -c pytorch` or\n",
    "`pip install pytorch torchvision`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First time download and install (i.e. uncomment the `!pip install` command) giving e.g.:\n",
    "```\n",
    "Collecting torch\n",
    "  Downloading torch-1.8.0-cp38-none-macosx_10_9_x86_64.whl (119.6 MB)\n",
    "     |████████████████████████████████| 119.6 MB 705 kB/s eta 0:00:01\n",
    "Collecting torchvision\n",
    "  Downloading torchvision-0.9.0-cp38-cp38-macosx_10_9_x86_64.whl (13.2 MB)\n",
    "     |████████████████████████████████| 13.2 MB 1.0 MB/s eta 0:00:01\n",
    "Requirement already satisfied: numpy in /Users/arvid/opt/anaconda3/envs/bmed360v2021/lib/python3.8/site-packages (from torch) (1.19.2)\n",
    "Collecting typing-extensions\n",
    "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
    "Requirement already satisfied: pillow>=4.1.1 in /Users/arvid/opt/anaconda3/envs/bmed360v2021/lib/python3.8/site-packages (from torchvision) (8.0.1)\n",
    "Installing collected packages: typing-extensions, torch, torchvision\n",
    "Successfully installed torch-1.8.0 torchvision-0.9.0 typing-extensions-3.7.4.3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tensorly as tl\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.8.0\n",
      "Tensorly version: 0.5.1\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version: {}'.format(torch.__version__))\n",
    "print('Tensorly version: {}'.format(tl.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are higher order extensions of matrices that can encode multi-dimensional structure in the data\n",
    "\n",
    "![tensor_illustration](../assets/tensor_cartoon.jpg)\n",
    "\n",
    "In this tutorial we will show how to manipulate tensors as ndarrays, using [TensorLy](http://tensorly.github.io) with the NumPy backend to perform tensor operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Creating a tensor\n",
    "\n",
    "A tensor can be represented in multiple ways. The simplest is the slice representation through multiple matrices.\n",
    "\n",
    "Let's take for this example the tensor $\\tilde X$ defined by its frontal slices:\n",
    "\n",
    "$$\n",
    "   X_1 = \n",
    "   \\left[\n",
    "   \\begin{matrix}\n",
    "   0  & 2  & 4  & 6\\\\\n",
    "   8  & 10 & 12 & 14\\\\\n",
    "   16 & 18 & 20 & 22\n",
    "   \\end{matrix}\n",
    "   \\right]\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "   X_2 =\n",
    "   \\left[\n",
    "   \\begin{matrix}\n",
    "   1  & 3  & 5  & 7\\\\\n",
    "   9  & 11 & 13 & 15\\\\\n",
    "   17 & 19 & 21 & 23\n",
    "   \\end{matrix}\n",
    "   \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In Python, this array can be expressed as a numpy array::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tl.tensor(np.arange(24).reshape((3, 4, 2)), dtype=tl.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  1.],\n",
       "        [ 2.,  3.],\n",
       "        [ 4.,  5.],\n",
       "        [ 6.,  7.]],\n",
       "\n",
       "       [[ 8.,  9.],\n",
       "        [10., 11.],\n",
       "        [12., 13.],\n",
       "        [14., 15.]],\n",
       "\n",
       "       [[16., 17.],\n",
       "        [18., 19.],\n",
       "        [20., 21.],\n",
       "        [22., 23.]]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the frontal slices by fixing the last axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  2.,  4.,  6.],\n",
       "       [ 8., 10., 12., 14.],\n",
       "       [16., 18., 20., 22.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  3.,  5.,  7.],\n",
       "       [ 9., 11., 13., 15.],\n",
       "       [17., 19., 21., 23.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, :, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Setting the backend\n",
    "\n",
    "`tl.set_backend('pytorch') # Or 'mxnet', 'numpy', 'tensorflow', 'cupy' or 'jax'`\n",
    "\n",
    "In TensorLy you can dynamically set the backend to use either NumPy or MXNet to represent tensors and perform the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the backend is set to NumPy, here is how to change it to PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tl.tensor(np.arange(24).reshape((3, 4, 2)))\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change it back to NumPy for the rest of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.set_backend('numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tl.tensor(np.arange(24).reshape((3, 4, 2)))\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Basic tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Unfolding\n",
    "\n",
    "Also called **matrization**, **unfolding** a tensor is done by reading the element in a given way as to obtain a matrix instead of a tensor.\n",
    "\n",
    "It is done by stacking the **fibers** of the tensor into a matrix.\n",
    "\n",
    "![tensor_illustration](../assets/fibers.png)\n",
    "Illustration: *Nonnegative Matrix and Tensor Factorizations*, Andrzej Cichocki, Rafal Zdunek, Anh Huy Phan, and Shun-ichi Amari, John Wiley & Sons, 2009.\n",
    "\n",
    "\n",
    "\n",
    "### Definition\n",
    "For a tensor of size $(I_1, I_2, \\cdots, I_n)$, the k-mode unfolding of this tensor will be of size $(I_k, I_1 \\times \\cdots \\times I_{k-1} \\times I_{k+1} \\cdots \\times I_n)$.\n",
    "\n",
    "\n",
    "   Given a tensor $\\tilde X \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\cdots \\times I_N}$, the\n",
    "   mode-n unfolding of $\\tilde X$ is a matrix $\\mathbf{X}_{[n]} \\in \\mathbb{R}^{I_n, I_M}$,\n",
    "   with $M = \\prod\\limits_{\\substack{k=1,\\\\k \\neq n}}^N I_k$ and is defined by\n",
    "   the mapping from element $(i_1, i_2, \\cdots, i_N)$ to $(i_n, j)$, with\n",
    "\n",
    "\n",
    "$$\n",
    "    j = \\sum\\limits_{\\substack{k=1,\\\\k \\neq n}}^N i_k \\times \\prod_{m=k+1}^N I_m.\n",
    "$$\n",
    "\n",
    "### Convention\n",
    "\n",
    "   Traditionally, mode-1 unfolding denotes the unfolding along the first dimension.\n",
    "   However, to be consistent with the Python indexing that always starts at zero,\n",
    "   in tensorly, unfolding also starts at zero!\n",
    "\n",
    "   Therefore ``unfold(tensor, 0)`` will unfold said tensor along its first dimension!\n",
    "   \n",
    "\n",
    "### Example\n",
    "\n",
    "For instance, using the $\\tilde X$ previously defined:\n",
    "$$\n",
    "   X_1 = \n",
    "   \\left[\n",
    "   \\begin{matrix}\n",
    "   0  & 2  & 4  & 6\\\\\n",
    "   8  & 10 & 12 & 14\\\\\n",
    "   16 & 18 & 20 & 22\n",
    "   \\end{matrix}\n",
    "   \\right]\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "   X_2 =\n",
    "   \\left[\n",
    "   \\begin{matrix}\n",
    "   1  & 3  & 5  & 7\\\\\n",
    "   9  & 11 & 13 & 15\\\\\n",
    "   17 & 19 & 21 & 23\n",
    "   \\end{matrix}\n",
    "   \\right]\n",
    "$$\n",
    "\n",
    "The 0-mode unfolding of $\\tilde X$:\n",
    "\n",
    "$$\n",
    "   \\tilde X_{[0]} =\n",
    "   \\left[ \\begin{matrix}\n",
    "      0 & 1 & 2 & 3 & 4 & 5 & 6 & 7\\\\\n",
    "      8 & 9 & 10 & 11 & 12 & 13 & 14 & 15\\\\\n",
    "      16 & 17 & 18 & 19 & 20 & 21 & 22 & 23\\\\\n",
    "   \\end{matrix} \\right]\n",
    "$$\n",
    "\n",
    "The 1-mode unfolding is given by:\n",
    "\n",
    "$$\n",
    "   \\tilde X_{[1]} =\n",
    "   \\left[ \\begin{matrix}\n",
    "      0 & 1 & 8 & 9 & 16 & 17\\\\\n",
    "      2 & 3 & 10 & 11 & 18 & 19\\\\\n",
    "      4 & 5 & 12 & 13 & 20 & 21\\\\\n",
    "      6 & 7 & 14 & 15 & 22 & 23\\\\\n",
    "   \\end{matrix} \\right]\n",
    "$$\n",
    "\n",
    "Finally, the 2-mode unfolding is the unfolding along the last axis:\n",
    "\n",
    "$$\n",
    "   \\tilde X_{[2]} =\n",
    "   \\left[ \\begin{matrix}\n",
    "      0 & 2 & 4 & 6 & 8 & 10 & 12 & 14 & 16 & 18 & 20 & 22\\\\\n",
    "      1 & 3 & 5 & 7 & 9 & 11 & 13 & 15 & 17 & 19 & 21 & 23\\\\\n",
    "   \\end{matrix} \\right]\n",
    "$$\n",
    "\n",
    "### In TensorLy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
       "       [16, 17, 18, 19, 20, 21, 22, 23]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.unfold(X, mode=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  8,  9, 16, 17],\n",
       "       [ 2,  3, 10, 11, 18, 19],\n",
       "       [ 4,  5, 12, 13, 20, 21],\n",
       "       [ 6,  7, 14, 15, 22, 23]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.unfold(X, mode=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22],\n",
       "       [ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19, 21, 23]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.unfold(X, mode=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Folding\n",
    "\n",
    "Folding is the inverse operation: you can **fold** an unfolded tensor back from matrix to full tensor using the ``tensorly.fold`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7]],\n",
       "\n",
       "       [[ 8,  9],\n",
       "        [10, 11],\n",
       "        [12, 13],\n",
       "        [14, 15]],\n",
       "\n",
       "       [[16, 17],\n",
       "        [18, 19],\n",
       "        [20, 21],\n",
       "        [22, 23]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfolding = tl.unfold(X, 1)\n",
    "original_shape = X.shape\n",
    "tl.fold(unfolding, mode=1, shape=original_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 n-mode product\n",
    "\n",
    "Also known as **tensor contraction**. This is a natural generalization of matrix-vector and matrix-matrix product. When multiplying a tensor by a matrix or a vector, we now have to specify the **mode** $n$ along which to take the product.\n",
    "### Tensor times matrix\n",
    "\n",
    "In that case we are doing an operation analogous to a matrix multiplication on the $n$-th mode. Given a tensor $\\tilde X$ of size $(I_1, I_2, \\cdots, I_N)$, and a matrix $M$ of size $(D, I_n)$, the $n$-mode product of $\\tilde X$ by $M$ is written $\\tilde X \\times_n M$ and is of size $(I_k, I_1 \\times \\cdots \\times I_{n-1} \\times D \\times I_{n+1} \\cdots \\times I_n)$.\n",
    "\n",
    "### Tensor times vector\n",
    "\n",
    "In that case we are contracting over the $n$-th mode by multiplying it with a vector. Given a tensor $\\tilde X$ of size $(I_1, I_2, \\cdots, I_N)$, and a vector $v$ of size $(I_n)$, the $n$-mode product of $\\tilde X$ by $v$ is written $\\tilde X \\times_n v$ and is of size $(I_k, I_1 \\times \\cdots \\times I_{n-1} \\times I_{n+1} \\cdots \\times I_n)$.\n",
    "\n",
    "![tensor_illustration](../assets/tensor_contraction.png)\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "In TensorLy, all the tensor algebra functions are located in the `tensorly.tenalg` module. For the n-mode product, you will need to use the function `mode_dot` that works transparently for multiplying a tensor by a matrix or a vector along a given mode.\n",
    "\n",
    "#### Tensor times matrix\n",
    "\n",
    "With the tensor $\\tilde X$ of size (3, 4, 2) we defined previously, let's define a matrix M of size (5, 4) to multiply along the second mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "M = tl.tensor(np.arange(4*5).reshape((5, 4)))\n",
    "print(M.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind indexing starts at zero, so the second mode is represented by `mode=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tl.tenalg.mode_dot(X, M, mode=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the result is of shape (3, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor times vector\n",
    "\n",
    "Similarly, we can contract along the mode 1 with a vector of size 4 (our tensor is of size (3, 4, 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "v = tl.tensor(np.arange(4))\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7]],\n",
       "\n",
       "       [[ 8,  9],\n",
       "        [10, 11],\n",
       "        [12, 13],\n",
       "        [14, 15]],\n",
       "\n",
       "       [[16, 17],\n",
       "        [18, 19],\n",
       "        [20, 21],\n",
       "        [22, 23]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tl.tenalg.mode_dot(X, v, mode=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that X is of shape (3, 4, 2). We are here contracting over its 2nd mode. \n",
    "\n",
    "Since we have multiplied by a vector, we have effectively contracted out one mode of the tensor so the result will be of matrix of size (3, 2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 28,  34],\n",
       "       [ 76,  82],\n",
       "       [124, 130]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could form each column (fiber) of the result, by taking the dot product between the frontal slices of X and the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 28,  76, 124])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, :, 0] @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 34,  82, 130])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, :, 1] @ v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can form the whole product by concanating these as the column of the resulting matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 28,  34],\n",
       "       [ 76,  82],\n",
       "       [124, 130]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([X[:, :, 0] @ v, X[:, :, 1] @ v]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could equivalently use the mode-0 slices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28 34]\n",
      "[76 82]\n",
      "[124 130]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(X[i, ...].T @ v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can form the whole result by stacking these as the rows of the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 28,  34],\n",
       "       [ 76,  82],\n",
       "       [124, 130]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([X[i, ...].T @ v for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BMED360V2021",
   "language": "python",
   "name": "bmed360v2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
